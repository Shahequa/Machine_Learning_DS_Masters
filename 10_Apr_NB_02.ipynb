{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "478ab40c-186c-4137-83be-d0234054e66c",
   "metadata": {},
   "source": [
    "## Naive Bayes Assignment - 2\n",
    "**By Shahequa Modabbera**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8dd980-e9d6-4a3b-a6c5-9819f6aa13f6",
   "metadata": {},
   "source": [
    "#### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0229947-b6b4-4a5d-bcda-998c4ebb73fd",
   "metadata": {},
   "source": [
    "Ans) To solve this problem, we can use conditional probability. Let's define the following events:\n",
    "\n",
    "A: Employee uses the company's health insurance plan.\n",
    "\n",
    "S: Employee is a smoker.\n",
    "\n",
    "We are given:\n",
    "\n",
    "P(A) = 0.70 (probability that an employee uses the health insurance plan)\n",
    "\n",
    "P(S|A) = 0.40 (probability that an employee is a smoker given that they use the health insurance plan)\n",
    "\n",
    "We want to find:\n",
    "\n",
    "P(S|A) (probability that an employee is a smoker given that they use the health insurance plan)\n",
    "\n",
    "We can use Bayes' theorem to calculate the conditional probability:\n",
    "\n",
    "P(S|A) = P(A|S) * P(S) / P(A)\n",
    "\n",
    "We are not given the values of P(A|S) or P(S), so we cannot directly calculate P(S|A) using this formula.\n",
    "\n",
    "However, we can make an assumption about the relationship between the events A and S. Let's assume that the probability of being a smoker is the same for employees who use the health insurance plan and those who don't. In other words, we assume that P(S|A') = P(S), where A' represents the event that an employee does not use the health insurance plan.\n",
    "\n",
    "With this assumption, we can write:\n",
    "\n",
    "P(S|A) = P(A|S) * P(S) / P(A) = P(A|S) * P(S) / (P(A|S) * P(S) + P(A|S') * P(S'))\n",
    "\n",
    "Since we are not given the value of P(A|S') or P(S'), we cannot compute the exact probability. We would need additional information to make a precise calculation.\n",
    "\n",
    "However, if we assume that the probability of using the health insurance plan is the same for smokers and non-smokers (P(A|S) = P(A|S')), we can simplify the equation as follows:\n",
    "\n",
    "P(S|A) = P(A|S) * P(S) / (P(A|S) * P(S) + P(A|S) * P(S'))\n",
    "\n",
    "       = P(A|S) * P(S) / (P(A|S) * P(S) + P(A|S) * (1 - P(S)))\n",
    "\n",
    "Using the given values:\n",
    "\n",
    "P(A|S) = 0.40 (probability that an employee uses the health insurance plan given that they are a smoker)\n",
    "\n",
    "P(S) = 0.40 (probability that an employee is a smoker)\n",
    "\n",
    "P(S|A) = 0.40 * 0.40 / (0.40 * 0.40 + 0.40 * (1 - 0.40))\n",
    "\n",
    "       = 0.16 / (0.16 + 0.24)\n",
    "       \n",
    "       = 0.16 / 0.40\n",
    "       \n",
    "       = 0.4\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that they use the health insurance plan is 0.4 or 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b8d485-6e3d-4ea7-bf00-02763e547d50",
   "metadata": {},
   "source": [
    "#### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e4948-6ace-49de-aebc-9c9f4c91ce02",
   "metadata": {},
   "source": [
    "Ans) The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of data they are suited for.\n",
    "\n",
    "1. Bernoulli Naive Bayes:\n",
    "   - Bernoulli Naive Bayes is used when the features are binary or follow a Bernoulli distribution, meaning they can take only two values (0 or 1).\n",
    "   - It assumes that each feature is independent of each other given the class.\n",
    "   - It is commonly used for text classification tasks, where the presence or absence of words in a document is used as features.\n",
    "\n",
    "2. Multinomial Naive Bayes:\n",
    "   - Multinomial Naive Bayes is used when the features are categorical or discrete, but can have more than two values.\n",
    "   - It assumes that the features follow a multinomial distribution, where the probability of an event is calculated as the count of occurrences divided by the total count of all events.\n",
    "   - It is often used in text classification tasks, where the features represent word frequencies or document term frequencies.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes is suitable for binary features, while Multinomial Naive Bayes is appropriate for discrete features with more than two possible values. The choice between the two depends on the nature of the data and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18362d1e-3b47-47c9-ada4-0af8da479cfe",
   "metadata": {},
   "source": [
    "#### Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859aaee1-3a8c-45ac-8f4e-26f1aa67c8b9",
   "metadata": {},
   "source": [
    "Ans) Bernoulli Naive Bayes does not handle missing values explicitly. When there are missing values in the dataset, they are typically treated as a separate category or considered as an indicator variable.\n",
    "\n",
    "Here are two common approaches to handle missing values in Bernoulli Naive Bayes:\n",
    "\n",
    "1. Separate category: Treat missing values as a separate category for each feature. This means that a missing value is considered as a distinct value that can be present or absent. The model will learn the probability of the missing value separately from the other categories.\n",
    "\n",
    "2. Indicator variable: Create an additional binary feature that indicates whether a value is missing or not. This approach adds a new feature that takes a value of 1 if the original feature is missing and 0 otherwise. The model then treats this indicator variable as a regular binary feature during training and classification.\n",
    "\n",
    "The choice between these approaches depends on the specific dataset and the nature of the missing values. It is important to consider the potential impact of missing values on the classification task and select the most appropriate handling method.\n",
    "\n",
    "Examples of how missing values can be handled in Bernoulli Naive Bayes:\n",
    "\n",
    "1. Separate category:\n",
    "   Let's say we have a dataset of emails with two features: \"contains word 'free'\" and \"contains word 'sale'\". The values for these features are binary: 1 if the word is present in the email and 0 if it is not. If there are missing values for these features, we can introduce a separate category, let's say -1, to represent the missing values. So, a missing value in the \"contains word 'free'\" feature would be encoded as -1, indicating that the presence or absence of the word is unknown for that email.\n",
    "\n",
    "2. Indicator variable:\n",
    "   Continuing with the email example, instead of introducing a separate category, we can create an additional binary feature called \"word missing\" to indicate whether a value is missing or not. This feature would take a value of 1 if either \"contains word 'free'\" or \"contains word 'sale'\" is missing, and 0 otherwise. So, for an email with a missing value in either of these features, the \"word missing\" feature would be set to 1, indicating that the presence or absence of either word is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f2350-d9ca-4d3f-b2e9-23ffd4837e2d",
   "metadata": {},
   "source": [
    "#### Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d3005e-0bd8-48f9-b49a-0c0dd0a11266",
   "metadata": {},
   "source": [
    "Ans) Yes, Gaussian Naive Bayes can be used for multi-class classification. It is one of the Naive Bayes variants commonly used for handling continuous or numeric features.\n",
    "\n",
    "In Gaussian Naive Bayes, it is assumed that the features follow a Gaussian (normal) distribution within each class. This assumption allows the algorithm to estimate the mean and standard deviation of each feature for each class. During prediction, the algorithm calculates the probability of an instance belonging to each class based on the Gaussian distribution parameters.\n",
    "\n",
    "For multi-class classification, Gaussian Naive Bayes can be extended to handle multiple classes by using the \"one-vs-all\" or \"one-vs-one\" strategies. \n",
    "\n",
    "In the \"one-vs-all\" strategy, a separate binary classifier is trained for each class, considering it as the positive class and the rest as the negative class. The class with the highest predicted probability is then assigned to the instance.\n",
    "\n",
    "In the \"one-vs-one\" strategy, a binary classifier is trained for each pair of classes. During prediction, each binary classifier votes for the class it assigns the instance to, and the class with the most votes is chosen as the predicted class.\n",
    "\n",
    "So, Gaussian Naive Bayes can handle multi-class classification by using these strategies to extend the binary classification framework to multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669263e5-1915-46d9-9d8b-337e32fce5d7",
   "metadata": {},
   "source": [
    "#### Q5. Assignment:\n",
    "#### Data preparation:\n",
    "#### Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "\n",
    "#### Implementation:\n",
    "#### Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "#### Results:\n",
    "#### Report the following performance metrics for each classifier:\n",
    "    Accuracy\n",
    "    Precision\n",
    "    Recall\n",
    "    F1 score\n",
    "\n",
    "#### Discussion:\n",
    "#### Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "#### Conclusion:\n",
    "#### Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "#### Note: This dataset contains a binary classification problem with multiple features. The dataset is relatively small, but it can be used to demonstrate the performance of the different variants of Naive Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4845cecf-088f-4aeb-b382-a8ce4e56addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier: Bernoulli Naive Bayes\n",
      "Accuracy: 0.8839\n",
      "Precision: 0.8870\n",
      "Recall: 0.8152\n",
      "F1 Score: 0.8481\n",
      "\n",
      "Classifier: Multinomial Naive Bayes\n",
      "Accuracy: 0.7863\n",
      "Precision: 0.7393\n",
      "Recall: 0.7215\n",
      "F1 Score: 0.7283\n",
      "\n",
      "Classifier: Gaussian Naive Bayes\n",
      "Accuracy: 0.8218\n",
      "Precision: 0.7104\n",
      "Recall: 0.9570\n",
      "F1 Score: 0.8131\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Download the Spambase dataset from the UCI Machine Learning Repository\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "data = pd.read_csv(url, header=None)\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Instantiate the classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Define the performance metrics to evaluate\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "# Perform cross-validation and evaluate performance metrics\n",
    "classifiers = {'Bernoulli Naive Bayes': bernoulli_nb, \n",
    "               'Multinomial Naive Bayes': multinomial_nb, \n",
    "               'Gaussian Naive Bayes': gaussian_nb}\n",
    "\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "    scores = cross_validate(classifier, X, y, cv=10, scoring=metrics)\n",
    "    mean_scores = {metric: np.mean(scores[f'test_{metric}']) for metric in metrics}\n",
    "    print(f\"\\nClassifier: {classifier_name}\")\n",
    "    print(f\"Accuracy: {mean_scores['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {mean_scores['precision']:.4f}\")\n",
    "    print(f\"Recall: {mean_scores['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {mean_scores['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a72c19-fe5a-43aa-b234-aadab903aa99",
   "metadata": {},
   "source": [
    "Ans) In this implementation, we first download the Spambase dataset from the UCI Machine Learning Repository using the provided URL. The dataset is then loaded into a pandas DataFrame, where the last column represents the target variable (spam or not spam) and the preceding columns are the input features.\n",
    "\n",
    "We then instantiate three Naive Bayes classifiers: BernoulliNB for binary features, MultinomialNB for discrete features, and GaussianNB for continuous features.\n",
    "\n",
    "Using 10-fold cross-validation, we evaluate the performance of each classifier by calculating the mean scores for the accuracy, precision, recall, and F1 score metrics. The `cross_val_score` function performs the cross-validation, and we pass the metrics to the `scoring` parameter to specify the metrics to compute.\n",
    "\n",
    "Finally, we print the performance metrics for each classifier.\n",
    "\n",
    "Discussion:\n",
    "Based on the results obtained, we can analyze the performance of each Naive Bayes variant:\n",
    "\n",
    "- Bernoulli Naive Bayes: It performs well on binary features where the presence or absence of a feature is considered. It may have worked well if the dataset has many binary features.\n",
    "\n",
    "- Multinomial Naive Bayes: It is suitable for discrete features where the feature represents counts or frequencies. This variant may have performed well if the dataset contains discrete features such as word counts or frequencies of specific words in the email messages.\n",
    "\n",
    "- Gaussian Naive Bayes: It assumes that the features follow a Gaussian distribution. However, this dataset contains a mixture of binary, discrete, and continuous features. Gaussian Naive Bayes assumes continuous features, so it may not have performed as well as the other variants in this case.\n",
    "\n",
    "From the results, it is important to note that the performance of each variant depends on the nature of the dataset and the specific characteristics of the features. In this case, either Bernoulli or Multinomial Naive Bayes may have performed better since the dataset consists of binary and discrete features.\n",
    "\n",
    "Limitations of Naive Bayes include its strong independence assumption, which may not hold in all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
