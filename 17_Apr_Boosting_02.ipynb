{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2d5ddd-3ed6-41d2-a067-f1cb39b8c195",
   "metadata": {},
   "source": [
    "## Boosting Assignment - 2\n",
    "**By Shahequa Modabbera**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0e1a2-ae1f-4622-a728-7f14ecc41370",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a74fe8-8c79-4372-aae8-e6c6bb0826df",
   "metadata": {},
   "source": [
    "Ans) Gradient Boosting Regression, also known as Gradient Boosted Regression Trees (GBRT), is a machine learning algorithm that combines the concepts of boosting and regression. It is a powerful technique for solving regression problems and is widely used in various domains.\n",
    "\n",
    "In Gradient Boosting Regression, the algorithm iteratively builds an ensemble of weak regression models (usually decision trees) in a stage-wise fashion. It trains each new model to correct the mistakes made by the previous models in the ensemble. The algorithm learns by minimizing a loss function, which measures the difference between the predicted and actual values of the target variable.\n",
    "\n",
    "The key idea behind Gradient Boosting Regression is that each weak learner is trained to address the weaknesses of the previous models, focusing on the remaining errors. This iterative process allows the algorithm to learn complex relationships and make accurate predictions.\n",
    "\n",
    "Gradient Boosting Regression offers several benefits, including its ability to handle a variety of data types, handle missing values, and provide robust predictions. However, it can be sensitive to noise and overfitting if not properly regularized.\n",
    "\n",
    "The scikit-learn library in Python provides an implementation of Gradient Boosting Regression called GradientBoostingRegressor, which can be readily used for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008aca89-9f98-428c-a744-1d7b61415d0d",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97b40b9-63e1-4bc2-a528-cd4cbb00e1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 31.735482349161565\n",
      "R-squared: 0.9772379183627112\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the predictions with the mean of y\n",
    "        y_pred = np.mean(y) * np.ones_like(y)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate the negative gradient (residuals)\n",
    "            residuals = y - y_pred\n",
    "            \n",
    "            # Fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update the predictions by adding the tree's predictions\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "            # Add the tree to the ensemble\n",
    "            self.estimators.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Initialize the predictions with zeros\n",
    "        y_pred = np.zeros(len(X))\n",
    "        \n",
    "        for tree in self.estimators:\n",
    "            # Add the tree's predictions to the ensemble predictions\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "# Usage example\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate a simple regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93373910-2c02-4429-963c-635d8c62e257",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f55aef48-0ed4-498e-aa1b-76519c12c1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 150}\n",
      "Mean Squared Error: 32.456435181317\n",
      "R-squared: 0.9767208193301015\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class GradientBoostingRegressor(BaseEstimator):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.mean(y) * np.ones_like(y)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - y_pred\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "            self.estimators.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        \n",
    "        for tree in self.estimators:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return r2_score(y, y_pred)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'max_depth': self.max_depth\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "# Generate a simple regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Create an instance of the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Create and fit the gradient boosting regressor with the best hyperparameters\n",
    "gb_regressor = GradientBoostingRegressor(**best_params)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = gb_regressor.score(X_test, y_test)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff1a0d6-3d37-4903-882f-94af83aa0817",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d8999-cc32-4d37-a438-8d8ff0b9fa19",
   "metadata": {},
   "source": [
    "Ans) In Gradient Boosting, a weak learner refers to a model that is relatively simple and performs slightly better than random guessing. It is typically a decision tree with a small number of nodes or a shallow depth. Unlike traditional decision trees that aim to create the best splits at each node, weak learners focus on capturing patterns or trends in the data that can be exploited to make better predictions.\n",
    "\n",
    "The key characteristic of a weak learner is that it should be better than random guessing on a particular task, but its individual performance may be limited. However, when combined with other weak learners through the boosting process, they can collectively form a strong learner that achieves high predictive accuracy.\n",
    "\n",
    "In Gradient Boosting, the weak learner models are trained sequentially, with each subsequent model trying to correct the mistakes made by the previous models. The ensemble of these weak learners, weighted according to their performance, forms the final boosted model.\n",
    "\n",
    "By iteratively adding weak learners and adjusting their weights, Gradient Boosting can effectively capture complex patterns and dependencies in the data, leading to improved predictive performance compared to using a single strong learner.\n",
    "\n",
    "Examples of weak learners used in Gradient Boosting include decision stumps (decision trees with only one split), shallow decision trees with limited depth, or linear models with restricted complexity.\n",
    "\n",
    "The strength of Gradient Boosting lies in its ability to combine multiple weak learners to create a powerful predictive model that can generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b2e5c9-c2e2-42b7-932a-3064f6e9a26c",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c037156a-a113-4328-ba24-a498912df7b9",
   "metadata": {},
   "source": [
    "Ans) The intuition behind the Gradient Boosting algorithm is to iteratively build an ensemble of weak learners that work together to improve the overall predictive performance. The algorithm focuses on minimizing the errors made by the previous weak learners and gradually refining the model.\n",
    "\n",
    "Here's a step-by-step intuition behind the Gradient Boosting algorithm:\n",
    "\n",
    "1. Start with an initial model: The algorithm begins with an initial weak learner, which can be a simple model like a decision tree with shallow depth.\n",
    "\n",
    "2. Fit the model to the data: The initial model is trained on the training data to make predictions. These predictions are compared to the actual target values, and the errors (residuals) are calculated.\n",
    "\n",
    "3. Train the next model to correct the errors: The next weak learner is trained to predict the residuals (errors) made by the previous model. The goal is to find a model that can effectively correct the mistakes of the previous model.\n",
    "\n",
    "4. Update the ensemble: The predictions of the new model are combined with the predictions of the previous models. Each prediction is weighted according to the performance of its corresponding weak learner.\n",
    "\n",
    "5. Repeat steps 2-4: The process is repeated iteratively, with each new model attempting to minimize the remaining errors from the previous models. The ensemble is updated at each iteration.\n",
    "\n",
    "6. Final prediction: The final prediction is obtained by summing the predictions from all the weak learners in the ensemble. This combined prediction aims to provide a more accurate and refined estimate of the target variable.\n",
    "\n",
    "The intuition behind Gradient Boosting is that by sequentially adding models that focus on correcting the mistakes of the previous models, the algorithm can gradually reduce the overall prediction error. It creates a strong ensemble model that can capture complex patterns and dependencies in the data, leading to improved predictive performance.\n",
    "\n",
    "Overall, the key idea of Gradient Boosting is to iteratively build a powerful model by leveraging the strengths of multiple weak learners, allowing them to compensate for each other's weaknesses and produce a more accurate and robust prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c70eeba-6961-485e-8c3f-351947598479",
   "metadata": {},
   "source": [
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf3e71-0cf7-4e0d-b8d3-64219c046618",
   "metadata": {},
   "source": [
    "Ans) The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative manner. Each weak learner is trained to correct the mistakes made by the previous learners, gradually improving the overall performance of the ensemble. Here's how the algorithm builds the ensemble:\n",
    "\n",
    "1. Start with an initial prediction: The algorithm starts with an initial prediction, which can be a simple estimate like the mean of the target variable. This serves as the base prediction for the first weak learner.\n",
    "\n",
    "2. Calculate the residuals: The residuals are the differences between the actual target values and the current prediction of the ensemble. For the first iteration, the residuals are simply the differences between the actual values and the initial prediction.\n",
    "\n",
    "3. Train a weak learner on the residuals: The next step is to train a weak learner, such as a decision tree with shallow depth, on the residuals. The weak learner's objective is to capture the patterns and relationships in the data that are not accounted for by the current ensemble.\n",
    "\n",
    "4. Update the ensemble: The weak learner's predictions are then combined with the predictions of the previous weak learners in the ensemble. The predictions are weighted based on the performance of each weak learner. The weights are determined using an optimization algorithm that minimizes a loss function, typically the mean squared error.\n",
    "\n",
    "5. Adjust the residuals: The residuals are updated by subtracting the predictions made by the latest weak learner. This adjustment brings the ensemble closer to the true target values.\n",
    "\n",
    "6. Repeat steps 3-5: The process is repeated for a predetermined number of iterations or until a specified stopping criterion is met. At each iteration, a new weak learner is trained on the updated residuals, the ensemble is updated, and the residuals are adjusted.\n",
    "\n",
    "7. Final prediction: The final prediction of the Gradient Boosting ensemble is obtained by summing the predictions of all the weak learners, typically with some learning rate or shrinkage factor to control the contribution of each weak learner.\n",
    "\n",
    "By iteratively adding weak learners that focus on the remaining errors or residuals, the Gradient Boosting algorithm gradually improves the ensemble's predictive performance. Each weak learner contributes to the ensemble by capturing different patterns and making corrections to the previous predictions. The ensemble combines the strengths of these weak learners to produce a more accurate and robust prediction.\n",
    "\n",
    "Overall, the Gradient Boosting algorithm builds an ensemble of weak learners by sequentially training and combining them to correct the errors made by the previous learners. This iterative process leads to the creation of a powerful model that can capture complex patterns and dependencies in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305c6995-802b-48f1-943c-3eb93260f434",
   "metadata": {},
   "source": [
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b125827-d84e-4e58-9738-697c3d011225",
   "metadata": {},
   "source": [
    "Ans) To construct the mathematical intuition of the Gradient Boosting algorithm, we can break it down into the following steps:\n",
    "\n",
    "1. Initialize the model: Start with an initial prediction or estimate, which serves as the base model. This could be a simple value like the mean of the target variable.\n",
    "\n",
    "2. Calculate the residuals: Compute the residuals by taking the differences between the actual target values and the current prediction of the model.\n",
    "\n",
    "3. Train a weak learner: Fit a weak learner, such as a decision tree with shallow depth, on the residuals. The weak learner's objective is to capture the patterns and relationships in the data that are not accounted for by the current model.\n",
    "\n",
    "4. Update the model: Update the current model by combining its predictions with the predictions made by the weak learner. The combination is weighted based on the performance of the weak learner. This weight is determined by an optimization algorithm that minimizes a loss function, typically the mean squared error.\n",
    "\n",
    "5. Adjust the residuals: Update the residuals by subtracting the predictions made by the weak learner. This adjustment brings the model closer to the true target values.\n",
    "\n",
    "6. Repeat steps 3-5: Repeat the process of training a weak learner, updating the model, and adjusting the residuals for a predetermined number of iterations or until a stopping criterion is met. At each iteration, a new weak learner is trained on the updated residuals, and the model and residuals are updated accordingly.\n",
    "\n",
    "7. Final prediction: The final prediction of the Gradient Boosting algorithm is obtained by summing the predictions of all the weak learners, typically with some learning rate or shrinkage factor to control the contribution of each weak learner.\n",
    "\n",
    "Mathematically, the intuition behind the Gradient Boosting algorithm lies in the optimization of a loss function by iteratively adding weak learners that focus on the remaining errors or residuals. The algorithm aims to find the optimal combination of weak learners that minimizes the overall loss and improves the predictive performance of the model.\n",
    "\n",
    "By following these steps, the Gradient Boosting algorithm creates an ensemble of weak learners that collectively work to capture complex patterns and dependencies in the data. Each weak learner contributes its own expertise to the ensemble, allowing the model to make more accurate predictions over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
