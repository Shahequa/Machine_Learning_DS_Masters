{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ff3c70-f0c4-460d-8372-7178dcbd5c2d",
   "metadata": {},
   "source": [
    "## KNN Assignment - 1\n",
    "**By Shahequa Modabbera**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74733964-a0e9-401b-a78f-1ab038e6a3eb",
   "metadata": {},
   "source": [
    "### Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b769c-3ecd-4ae3-a656-a9474ccc3c7a",
   "metadata": {},
   "source": [
    "Ans) The K-Nearest Neighbors (KNN) algorithm is a simple yet powerful supervised machine learning algorithm used for both classification and regression tasks. It is a non-parametric algorithm, meaning it does not make any assumptions about the underlying data distribution. Instead, it makes predictions based on the similarity between new data points and the labeled data points in the training set.\n",
    "\n",
    "In the KNN algorithm, the \"K\" refers to the number of nearest neighbors that are considered for making predictions. Here's how the KNN algorithm works:\n",
    "\n",
    "1. Load the dataset: Start by loading the labeled dataset, which consists of input features and corresponding target labels.\n",
    "\n",
    "2. Choose the value of K: Determine the number of nearest neighbors (K) to consider for making predictions. It should be a positive integer value.\n",
    "\n",
    "3. Calculate distances: For each new data point, calculate the distance between that point and all other data points in the training set. Common distance metrics used include Euclidean distance, Manhattan distance, or cosine distance.\n",
    "\n",
    "4. Find K nearest neighbors: Select the K data points from the training set that are closest to the new data point based on the calculated distances.\n",
    "\n",
    "5. Make predictions: For classification tasks, the majority class among the K nearest neighbors is assigned as the predicted class for the new data point. For regression tasks, the predicted value is the average (mean) of the target values of the K nearest neighbors.\n",
    "\n",
    "6. Evaluate performance: Measure the performance of the KNN model using appropriate evaluation metrics such as accuracy, precision, recall, F1 score, or mean squared error (MSE), depending on the problem type.\n",
    "\n",
    "7. Repeat steps 3-6: Optionally, we can repeat steps 3-6 with different values of K or try different distance metrics to find the optimal configuration that yields the best performance.\n",
    "\n",
    "The KNN algorithm is easy to understand and implement, and it can handle multi-class classification problems. However, it can be computationally expensive when dealing with large datasets, as it requires calculating distances for each new data point. Additionally, the choice of K and the distance metric can significantly impact the algorithm's performance, so parameter tuning and feature scaling may be necessary for optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7565e5-2fce-4838-8ef0-48b0cd8f1a05",
   "metadata": {},
   "source": [
    "### Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf5b82-92ce-4ce7-8130-2548de4f0d1b",
   "metadata": {},
   "source": [
    "Ans) Choosing the value of K in the K-Nearest Neighbors (KNN) algorithm is an important consideration that can impact the model's performance. The selection of K should be based on the characteristics of the dataset and the problem at hand. Here are a few approaches to choose an appropriate value for K:\n",
    "\n",
    "1. Domain knowledge: If we have prior knowledge about the problem domain, we can use that knowledge to guide our choice of K. For example, if we know that the classes in our dataset are well-separated, choosing a small value of K may be appropriate. On the other hand, if the classes overlap or have complex boundaries, a larger value of K might be better.\n",
    "\n",
    "2. Square root rule: One common heuristic is to take the square root of the number of samples in our dataset and use that as the value of K. This rule provides a balance between capturing sufficient local information and avoiding overfitting. However, it is just a rule of thumb and may not always be optimal.\n",
    "\n",
    "3. Cross-validation: Use cross-validation techniques to evaluate the performance of the KNN algorithm with different values of K. Split our dataset into training and validation sets, and iterate over a range of K values. For each value of K, train the KNN model on the training set and evaluate its performance on the validation set using appropriate metrics. Choose the value of K that gives the best performance.\n",
    "\n",
    "4. Grid search: Perform an exhaustive search over a predefined range of K values using a grid search approach. Train and evaluate the KNN model for each value of K and select the one that yields the best performance on a separate validation set or using cross-validation.\n",
    "\n",
    "5. Consider dataset size: The size of our dataset can also influence the choice of K. If we have a small dataset, choosing a small value of K may lead to overfitting, while a larger dataset can tolerate a larger value of K. It is generally recommended to choose K such that the number of neighbors is much smaller than the number of samples.\n",
    "\n",
    "It's important to note that there is no one-size-fits-all value of K that works for every problem. The choice of K depends on the specific characteristics of our dataset, the complexity of the problem, and the available computational resources. Experimentation and evaluation with different values of K are essential to determine the optimal value for our particular task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02853e05-dd85-4147-8f3d-b8dbac04d2f6",
   "metadata": {},
   "source": [
    "### Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d92a8e-8ce4-40fe-a621-bd2f4e9861b1",
   "metadata": {},
   "source": [
    "Ans) The main difference between the K-Nearest Neighbors (KNN) classifier and KNN regressor lies in the type of problem they are designed to solve.\n",
    "\n",
    "KNN Classifier:\n",
    "The KNN classifier is used for classification tasks, where the goal is to assign a categorical label or class to a given input sample. It works by finding the K nearest neighbors to the query point based on a distance metric (e.g., Euclidean distance) and assigning the class label based on majority voting among the neighbors. The predicted output is a discrete class label.\n",
    "\n",
    "KNN Regressor:\n",
    "The KNN regressor, on the other hand, is used for regression tasks, where the goal is to predict a continuous numerical value for a given input sample. Instead of assigning class labels, the KNN regressor estimates the output value by taking the average or weighted average of the target values of the K nearest neighbors. The predicted output is a continuous value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ab3dda-f460-4311-85df-6514bffffc9d",
   "metadata": {},
   "source": [
    "### Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977a0a5b-ad3b-4e8b-a84f-ce93c705ffc2",
   "metadata": {},
   "source": [
    "Ans) The performance of the K-Nearest Neighbors (KNN) algorithm can be evaluated using various performance metrics depending on the task at hand, such as classification or regression. Here are some commonly used metrics:\n",
    "\n",
    "1. Classification Metrics:\n",
    "   - Accuracy: It measures the overall correctness of the classifier by calculating the ratio of correctly classified instances to the total number of instances.\n",
    "   - Precision: It measures the proportion of true positive predictions out of all positive predictions, indicating the classifier's ability to avoid false positives.\n",
    "   - Recall: Also known as sensitivity or true positive rate, it measures the proportion of correctly predicted positive instances out of all actual positive instances.\n",
    "   - F1 Score: It combines precision and recall into a single metric, providing a balanced measure of the classifier's performance.\n",
    "\n",
    "2. Regression Metrics:\n",
    "   - Mean Squared Error (MSE): It calculates the average squared difference between the predicted values and the true values, giving an indication of the overall model accuracy.\n",
    "   - Mean Absolute Error (MAE): It calculates the average absolute difference between the predicted values and the true values, providing a measure of the average prediction error.\n",
    "   - R-squared: Also known as the coefficient of determination, it measures the proportion of the variance in the target variable that can be explained by the model. It ranges from 0 to 1, with higher values indicating a better fit.\n",
    "\n",
    "It's important to choose the appropriate performance metric based on the specific problem and the nature of the data. For example, accuracy may be suitable for balanced classification problems, while MSE or MAE are commonly used for regression tasks. Additionally, cross-validation techniques like k-fold cross-validation can be employed to get a more reliable estimate of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaed514-34b4-47fd-9e81-c48d3011c11d",
   "metadata": {},
   "source": [
    "### Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269ba53c-b2ef-4322-a3a6-68c95dcdde64",
   "metadata": {},
   "source": [
    "Ans) The \"curse of dimensionality\" refers to the challenges and limitations that arise when working with high-dimensional data in machine learning algorithms, including the K-Nearest Neighbors (KNN) algorithm. It describes the phenomenon where the performance and effectiveness of certain algorithms degrade as the number of input dimensions increases.\n",
    "\n",
    "In the context of KNN, the curse of dimensionality manifests in several ways:\n",
    "\n",
    "1. Increased sparsity: As the number of dimensions increases, the available data becomes increasingly sparse in the high-dimensional space. This sparsity makes it difficult to find neighboring data points that are close enough to provide meaningful similarity measures.\n",
    "\n",
    "2. Increased computational complexity: As the number of dimensions grows, the computational cost of finding the nearest neighbors increases exponentially. This is because the algorithm needs to calculate distances or similarities in each dimension for each data point, leading to a significant increase in computational time.\n",
    "\n",
    "3. Loss of discriminatory power: In high-dimensional spaces, the distances between data points tend to become more similar, making it harder to distinguish between similar and dissimilar instances. This can result in decreased discriminatory power and poorer classification performance.\n",
    "\n",
    "4. Overfitting: With a large number of dimensions, the KNN algorithm may become prone to overfitting the training data. The algorithm can effectively memorize the training instances, leading to reduced generalization performance on unseen data.\n",
    "\n",
    "To mitigate the curse of dimensionality in KNN and other algorithms, dimensionality reduction techniques, such as Principal Component Analysis (PCA) or feature selection, can be applied to reduce the number of dimensions while preserving relevant information. Additionally, careful feature engineering, data preprocessing, and regularization techniques can help address the challenges associated with high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67811cad-7941-4dec-a815-1490354d6d56",
   "metadata": {},
   "source": [
    "### Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b24b9-3390-4b67-a15f-ba1559c94008",
   "metadata": {},
   "source": [
    "Ans) Handling missing values in the K-Nearest Neighbors (KNN) algorithm can be approached in different ways depending on the specific dataset and the nature of the missing values. Here are a few common strategies:\n",
    "\n",
    "1. Deletion: One simple approach is to remove instances (rows) with missing values from the dataset. However, this may result in a loss of valuable information, especially if a large portion of the data is missing. Deletion is typically used when the missing values are considered to be missing completely at random (MCAR).\n",
    "\n",
    "2. Imputation: Another approach is to impute (fill in) the missing values with estimated values. Imputation methods aim to make reasonable guesses for the missing values based on the available data. Popular imputation techniques include mean imputation (replacing missing values with the mean of the feature), median imputation (replacing missing values with the median of the feature), or mode imputation (replacing missing values with the mode of the feature). Imputation is commonly used when the missing values are considered to be missing at random (MAR) or missing not at random (MNAR).\n",
    "\n",
    "3. Indicator variable: In some cases, it may be informative to introduce an indicator variable that represents whether a value is missing or not. This allows the KNN algorithm to treat missing values as a separate category, potentially capturing valuable patterns in the data.\n",
    "\n",
    "4. Weighted KNN: Another approach is to modify the KNN algorithm to assign weights to each neighbor based on their proximity and similarity to the target instance. The weights can be inversely proportional to the distance between instances, giving more importance to closer neighbors. This way, missing values in a particular feature would have less impact on the similarity calculation.\n",
    "\n",
    "The choice of how to handle missing values in KNN depends on the specific dataset, the percentage of missing values, the nature of the missingness, and the underlying problem. It's recommended to carefully analyze the dataset and consider the potential impact of each strategy before deciding on the most appropriate approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f033a2d4-87e1-4c19-b90b-e11c84ea8add",
   "metadata": {},
   "source": [
    "### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fafd85d-9e28-451a-9d31-4da36442e9db",
   "metadata": {},
   "source": [
    "Ans) The K-Nearest Neighbors (KNN) algorithm can be used for both classification and regression tasks. Here are the key differences between the performance of the KNN classifier and regressor:\n",
    "\n",
    "1. Output:\n",
    "   - KNN Classifier: The classifier predicts the class label of a new instance based on the majority class labels of its k nearest neighbors.\n",
    "   - KNN Regressor: The regressor predicts the numerical value of a new instance based on the average (or weighted average) of the target values of its k nearest neighbors.\n",
    "\n",
    "2. Evaluation:\n",
    "   - KNN Classifier: Classification performance is typically evaluated using metrics such as accuracy, precision, recall, and F1-score. These metrics measure the model's ability to correctly classify instances into different classes.\n",
    "   - KNN Regressor: Regression performance is usually evaluated using metrics such as mean squared error (MSE), root mean squared error (RMSE), or R-squared. These metrics assess the model's ability to estimate the continuous target variable accurately.\n",
    "\n",
    "3. Problem Type:\n",
    "   - KNN Classifier: The classifier is suitable for problems where the target variable is categorical or discrete. It is commonly used for problems such as image recognition, text classification, or customer segmentation.\n",
    "   - KNN Regressor: The regressor is appropriate for problems where the target variable is continuous or numerical. It is commonly used for problems such as predicting housing prices, stock market forecasting, or weather prediction.\n",
    "\n",
    "4. Handling Outliers:\n",
    "   - KNN Classifier: The classifier is generally robust to outliers since the class prediction is based on the majority vote of the nearest neighbors. Outliers may have a limited impact on the final classification decision.\n",
    "   - KNN Regressor: The regressor can be sensitive to outliers since the prediction is based on the average of the target values of the nearest neighbors. Outliers can significantly affect the average and lead to inaccurate predictions.\n",
    "\n",
    "The choice between the KNN classifier and regressor depends on the nature of the problem and the type of the target variable. If the target variable is categorical, the KNN classifier is suitable for classification tasks. If the target variable is continuous, the KNN regressor is suitable for regression tasks. It's important to consider the specific requirements of the problem and evaluate the performance of both approaches to determine which one is better for a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70b6c45-e4cd-4eed-9dc7-30604635ed9d",
   "metadata": {},
   "source": [
    "### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5c42a-16b0-48a1-aee7-bc772ef06370",
   "metadata": {},
   "source": [
    "Ans) The K-Nearest Neighbors (KNN) algorithm has several strengths and weaknesses for both classification and regression tasks. Here are some of them:\n",
    "\n",
    "Strengths of KNN:\n",
    "\n",
    "1. Intuitive and simple: KNN is easy to understand and implement. It does not make strong assumptions about the underlying data distribution.\n",
    "\n",
    "2. Non-parametric: KNN is a non-parametric algorithm, meaning it does not assume a specific form for the data. It can handle data with complex relationships and is suitable for both linear and non-linear problems.\n",
    "\n",
    "3. No training phase: KNN does not require an explicit training phase. The algorithm simply stores the training data and uses it for making predictions.\n",
    "\n",
    "4. Adaptability to new data: KNN can easily adapt to new training instances as they become available. It can incorporate new data without requiring a complete retraining of the model.\n",
    "\n",
    "Weaknesses of KNN:\n",
    "\n",
    "1. Computational complexity: KNN can be computationally expensive, especially for large datasets or high-dimensional data. As the number of training instances grows, the prediction time increases since it needs to calculate distances to all instances.\n",
    "\n",
    "2. Sensitivity to feature scaling: KNN is sensitive to the scale of features. If the features have different scales, those with larger magnitudes can dominate the distance calculations, leading to biased results. Feature scaling techniques like normalization or standardization can be applied to address this issue.\n",
    "\n",
    "3. Optimal choice of K: The value of K (the number of nearest neighbors) needs to be selected carefully. A small K may lead to high variance and overfitting, while a large K may result in high bias and underfitting. It is necessary to find the right balance through experimentation or using cross-validation techniques.\n",
    "\n",
    "4. Imbalanced datasets: KNN can struggle with imbalanced datasets where the number of instances in different classes is significantly different. The majority class can dominate the prediction, leading to biased results. Techniques like undersampling, oversampling, or using weighted KNN can be applied to address class imbalance.\n",
    "\n",
    "To address these weaknesses, some strategies can be employed:\n",
    "\n",
    "1. Efficient data structures: Various data structures like KD-trees or ball trees can be used to speed up the nearest neighbor search process and reduce computational complexity.\n",
    "\n",
    "2. Feature scaling: Applying feature scaling techniques, such as normalization or standardization, can help mitigate the impact of different feature scales on the distance calculations.\n",
    "\n",
    "3. Cross-validation: Cross-validation techniques, such as k-fold cross-validation, can be used to find an optimal value of K and assess the model's performance on unseen data.\n",
    "\n",
    "4. Ensemble methods: Combining multiple KNN models through ensemble methods, such as bagging or boosting, can help improve prediction accuracy and reduce overfitting.\n",
    "\n",
    "5. Handling imbalanced datasets: Applying techniques like oversampling the minority class, undersampling the majority class, or using weighted KNN can help address imbalanced datasets and improve the performance for minority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d483c3-bd20-4a5e-92d4-37d59183b544",
   "metadata": {},
   "source": [
    "### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa60a6b-ea25-406c-adc1-06a1b5226838",
   "metadata": {},
   "source": [
    "Ans) Euclidean distance and Manhattan distance are both distance metrics used in the K-Nearest Neighbors (KNN) algorithm to calculate the proximity between data points. Here are the main differences between the two:\n",
    "\n",
    "Euclidean Distance:\n",
    "- Also known as L2 distance, it calculates the straight-line distance between two points in Euclidean space.\n",
    "- It is computed as the square root of the sum of squared differences between corresponding coordinates of the two points.\n",
    "- The formula for Euclidean distance between two points (p1, q1) and (p2, q2) in a 2D space is: sqrt((p2 - p1)^2 + (q2 - q1)^2).\n",
    "- It considers the diagonal distance and takes into account the magnitude and direction of differences in each dimension.\n",
    "- It is suitable for continuous and numerical data.\n",
    "\n",
    "Manhattan Distance:\n",
    "- Also known as L1 distance or city block distance, it calculates the distance between two points by summing the absolute differences in their coordinates.\n",
    "- It is computed as the sum of absolute differences between corresponding coordinates of the two points.\n",
    "- The formula for Manhattan distance between two points (p1, q1) and (p2, q2) in a 2D space is: |p2 - p1| + |q2 - q1|.\n",
    "- It considers only the horizontal and vertical distance, disregarding diagonal movement.\n",
    "- It is suitable for discrete and categorical data or scenarios where movement can only happen in orthogonal directions.\n",
    "\n",
    "Differences and Use Cases:\n",
    "- Euclidean distance captures both magnitude and direction, making it suitable for scenarios where the directionality of differences matters, such as spatial or continuous data analysis.\n",
    "- Manhattan distance measures the total difference along each dimension independently and is often used when movement is restricted to horizontal and vertical directions, such as city block navigation or feature selection in high-dimensional spaces.\n",
    "\n",
    "The choice between Euclidean and Manhattan distance depends on the nature of the data and the problem at hand. It is common to try both metrics and evaluate their performance to determine which one provides better results in a given context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d919b18e-37f6-4f2f-9d43-52527af37ed7",
   "metadata": {},
   "source": [
    "### Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb05082-e182-4b7c-bc24-d29e5241e5c2",
   "metadata": {},
   "source": [
    "Ans) Feature scaling plays an important role in K-Nearest Neighbors (KNN) algorithm for the following reasons:\n",
    "\n",
    "1. Distance Calculation: KNN relies on the calculation of distances between data points to determine their proximity. If the features have different scales or units, the distance calculations can be dominated by features with larger scales, leading to biased results. Feature scaling helps to bring all the features to a similar scale, ensuring that each feature contributes proportionately to the distance calculations.\n",
    "\n",
    "2. Equal Weightage: KNN treats all features equally in determining the similarity between data points. If the features have different scales, the features with larger scales can have a greater impact on the overall distance, potentially overshadowing the contributions of other features. By scaling the features, each feature is given equal weightage, ensuring a fair comparison.\n",
    "\n",
    "3. Convergence and Efficiency: Feature scaling can improve the convergence speed and efficiency of the KNN algorithm. Scaling the features can lead to a more balanced search space, allowing the algorithm to converge faster. It can also make the algorithm more efficient by reducing the computational burden of calculating distances in high-dimensional spaces.\n",
    "\n",
    "Common methods for feature scaling in KNN include:\n",
    "\n",
    "- Min-Max Scaling (Normalization): It scales the features to a specific range, usually between 0 and 1, by subtracting the minimum value and dividing by the range (maximum value minus minimum value) of each feature.\n",
    "\n",
    "- Standardization (Z-score Scaling): It transforms the features to have zero mean and unit variance by subtracting the mean and dividing by the standard deviation of each feature.\n",
    "\n",
    "The choice of feature scaling method depends on the specific characteristics of the data and the requirements of the problem. It is generally recommended to apply feature scaling before using KNN to ensure fair and accurate comparisons between data points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
