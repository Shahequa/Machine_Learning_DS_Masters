{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199784ba-4bdb-4f61-8aad-9ba1dfab4f05",
   "metadata": {},
   "source": [
    "## Naive Bayes Assignment - 1\n",
    "**By Shahequa Modabbera**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c60079-1d4f-4521-9e53-a1f8c3de2dc6",
   "metadata": {},
   "source": [
    "### Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920549b-14ff-49c8-9e0e-b1499c467813",
   "metadata": {},
   "source": [
    "Ans) Bayes' theorem is a fundamental concept in probability theory and statistics that describes how to update or revise our beliefs about an event based on new evidence or information. It establishes a relationship between conditional probabilities, allowing us to calculate the probability of an event given certain conditions or prior knowledge.\n",
    "\n",
    "The theorem is named after Thomas Bayes, an English mathematician and Presbyterian minister, who formulated it in the 18th century. Bayes' theorem is expressed as follows:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "Where:\n",
    "- P(A|B) represents the conditional probability of event A given event B has occurred.\n",
    "- P(B|A) is the conditional probability of event B given event A has occurred.\n",
    "- P(A) and P(B) are the probabilities of events A and B, respectively.\n",
    "\n",
    "In simpler terms, Bayes' theorem states that the probability of event A occurring given that event B has occurred is equal to the probability of event B occurring given that event A has occurred, multiplied by the prior probability of event A, divided by the prior probability of event B.\n",
    "\n",
    "Bayes' theorem is particularly useful in situations where we have prior knowledge or assumptions about the probabilities involved and want to update those probabilities based on new evidence. It is commonly applied in fields such as statistics, machine learning, and Bayesian inference, where it plays a crucial role in decision-making, hypothesis testing, and predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e94c6-9cec-45e3-ba5f-d034b1e3c08a",
   "metadata": {},
   "source": [
    "### Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78088480-79de-452d-ace4-65a365cd4582",
   "metadata": {},
   "source": [
    "Ans) The formula for Bayes' theorem is as follows:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "Where:\n",
    "- P(A|B) represents the conditional probability of event A given event B has occurred.\n",
    "- P(B|A) is the conditional probability of event B given event A has occurred.\n",
    "- P(A) and P(B) are the probabilities of events A and B, respectively.\n",
    "\n",
    "In words, Bayes' theorem states that the probability of event A occurring given that event B has occurred is equal to the probability of event B occurring given that event A has occurred, multiplied by the prior probability of event A, divided by the prior probability of event B.\n",
    "\n",
    "Bayes' theorem is a fundamental concept in probability theory and statistics, and it provides a mathematical framework for updating probabilities based on new evidence or information. It is widely used in various fields, including Bayesian inference, machine learning, and decision-making, to make informed predictions and decisions based on available data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a66359-fd6d-479d-9021-112d2f51ab59",
   "metadata": {},
   "source": [
    "### Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2dbc98-4815-4dbd-be21-480e63b11bd9",
   "metadata": {},
   "source": [
    "Ans) Bayes' theorem is used in practice in various fields and applications. Here are a few examples:\n",
    "\n",
    "1. Bayesian Inference: Bayes' theorem is the cornerstone of Bayesian inference, a statistical framework for updating probabilities based on prior knowledge and new evidence. It allows for the incorporation of prior beliefs or information into the estimation of parameters or the prediction of outcomes.\n",
    "\n",
    "2. Spam Filtering: Bayes' theorem is commonly employed in spam filtering algorithms. By analyzing the content and characteristics of incoming emails, the algorithm calculates the probability that an email is spam given certain features (such as specific keywords or patterns). Bayes' theorem helps update these probabilities based on the presence or absence of those features in the email.\n",
    "\n",
    "3. Medical Diagnosis: Bayes' theorem plays a crucial role in medical diagnosis, particularly in situations where multiple symptoms and test results are involved. Given the symptoms exhibited by a patient, Bayes' theorem allows physicians to calculate the probability of a specific disease or condition being present, taking into account the prevalence of the disease and the accuracy of the diagnostic tests.\n",
    "\n",
    "4. Machine Learning: Bayes' theorem is utilized in various machine learning algorithms, such as Naive Bayes classifiers. These classifiers estimate the probability of a particular class label given the observed features of a data point. By leveraging Bayes' theorem, these algorithms can make predictions based on the calculated probabilities.\n",
    "\n",
    "5. Natural Language Processing: Bayes' theorem finds application in language modeling and natural language processing tasks. It helps determine the probability of a word or phrase occurring in a specific context or given a sequence of previous words. This information is then utilized in tasks such as language generation, machine translation, and speech recognition.\n",
    "\n",
    "Overall, Bayes' theorem provides a powerful framework for reasoning under uncertainty and updating probabilities based on available evidence. Its applications extend across various fields, enabling informed decision-making, prediction, and estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295a4066-e7cf-4e53-bfd0-fc840471752a",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e918911-c438-4447-ba54-068f03ce5511",
   "metadata": {},
   "source": [
    "Ans) Bayes' theorem is based on conditional probability and provides a way to calculate the reverse conditional probability given the conditional probability in the opposite direction. \n",
    "\n",
    "Conditional probability is the probability of an event A occurring given that another event B has already occurred, denoted as P(A|B). It quantifies the likelihood of A happening, taking into account the information provided by B.\n",
    "\n",
    "Bayes' theorem builds upon conditional probability and states the relationship between the reverse conditional probability, P(B|A), and the conditional probability, P(A|B). It can be mathematically expressed as:\n",
    "\n",
    "P(B|A) = (P(A|B) * P(B)) / P(A)\n",
    "\n",
    "In simpler terms, Bayes' theorem allows us to update our belief or estimate of the probability of an event occurring (B) based on new evidence or information (A). It provides a way to calculate the posterior probability (P(B|A)) by multiplying the likelihood of A given B (P(A|B)) with the prior probability of B (P(B)) and normalizing it by dividing by the probability of A (P(A)).\n",
    "\n",
    "Essentially, Bayes' theorem provides a framework to update probabilities based on new information, allowing us to revise our beliefs or make more informed decisions. It is widely used in statistics, machine learning, and various other fields for inference, prediction, and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec57054-ebd7-4ab5-a0a8-580bb65b7e57",
   "metadata": {},
   "source": [
    "### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46820bae-441a-42c0-84a1-111ce2621b84",
   "metadata": {},
   "source": [
    "Ans) When choosing which type of Naive Bayes classifier to use for a given problem, it is important to consider the characteristics of the problem and the assumptions made by each type of classifier. Here are some guidelines to help you make a decision:\n",
    "\n",
    "1. Gaussian Naive Bayes: This classifier assumes that the continuous features in the dataset follow a Gaussian (normal) distribution. It is suitable for problems where the continuous variables can be reasonably assumed to be normally distributed, such as when dealing with measurements like height or weight.\n",
    "\n",
    "2. Multinomial Naive Bayes: This classifier is specifically designed for problems with discrete features, particularly when the features represent the frequencies or counts of different outcomes. It is commonly used in text classification tasks, where the features can be word frequencies or document frequencies.\n",
    "\n",
    "3. Bernoulli Naive Bayes: This classifier is similar to the multinomial Naive Bayes, but it is specifically designed for binary features, where the features are either present (1) or absent (0). It is suitable for problems where the features are binary variables, such as document classification tasks where the presence or absence of certain words is used as features.\n",
    "\n",
    "The choice of the Naive Bayes classifier depends on the nature of the features in our dataset and the assumptions made by each type. If our features are continuous and normally distributed, Gaussian Naive Bayes is a good choice. For discrete features, multinomial or Bernoulli Naive Bayes can be used based on whether the features represent counts or binary values. However, it is important to note that these assumptions may not always hold in real-world datasets, so it is recommended to evaluate the performance of different Naive Bayes classifiers using cross-validation or other validation techniques to choose the most appropriate one for our specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e642b2b4-b6fc-4171-97e2-fdfe6cc4945d",
   "metadata": {},
   "source": [
    "### Q6. Assignment:\n",
    "    \n",
    "### You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:\n",
    "    \n",
    "     Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "\n",
    "        A   3    3    4    4    3    3   3\n",
    "\n",
    "        B   2    2    1    2    2    2   3\n",
    "\n",
    "### Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaac344-71d6-467d-8d37-97b1192bb2f2",
   "metadata": {},
   "source": [
    "Ans) To predict the class of a new instance using Naive Bayes, we calculate the conditional probability of each class given the feature values and select the class with the highest probability.\n",
    "\n",
    "The probabilities for the given new instance with features X1 = 3 and X2 = 4.\n",
    "\n",
    "For class A:\n",
    "\n",
    "P(A|X1=3, X2=4) = {P(X1=3|A) * P(X2=4|A) * P(A)} / {P(X1=3) * P(X2=4)}\n",
    "\n",
    "P(X1=3|A) = Frequency of X1=3 for class A / Total instances of class A\n",
    "            \n",
    "            = 4 / (3 + 3 + 4 + 4 + 3 + 3 + 3) = 4 / 23\n",
    "\n",
    "P(X2=4|A) = Frequency of X2=4 for class A / Total instances of class A\n",
    "            \n",
    "            = 3 / (3 + 3 + 4 + 4 + 3 + 3 + 3) = 3 / 23\n",
    "\n",
    "P(A) = Prior probability of class A\n",
    "       \n",
    "       = 1/2 (assuming equal prior probabilities for each class)\n",
    "\n",
    "P(A|X1=3, X2=4) = {(4 / 23) * (3 / 23) * (1/2)} [Since both the denominators are same , they will be cut out]\n",
    "                \n",
    "                = 0.011\n",
    "                \n",
    "Similarly, for class B:\n",
    "\n",
    "P(B|X1=3, X2=4) = {P(X1=3|B) * P(X2=4|B) * P(B)} / {P(X1=3) * P(X2=4)}\n",
    "\n",
    "P(X1=3|B) = Frequency of X1=3 for class B / Total instances of class B\n",
    "            \n",
    "            = 1 / (2 + 2 + 1 + 2 + 2 + 2 + 3) = 1 / 14\n",
    "\n",
    "P(X2=4|B) = Frequency of X2=4 for class B / Total instances of class B\n",
    "            \n",
    "            = 3 / (2 + 2 + 1 + 2 + 2 + 2 + 3) = 3 / 14\n",
    "\n",
    "P(B) = Prior probability of class B\n",
    "       \n",
    "       = 1/2 (assuming equal prior probabilities for each class)\n",
    "\n",
    "P(B|X1=3, X2=4) = (1 / 14) * (3 / 14) * (1/2) [Since both the denominators are same , they will be cut out]\n",
    "                \n",
    "                =0.007\n",
    "                \n",
    "Finally,\n",
    "\n",
    "P(A|X1=3, X2=4) = 0.011 / (0.011 + 0.007)\n",
    "                \n",
    "                = 0.61\n",
    "                \n",
    "                = 61%\n",
    "\n",
    "P(B|X1=3, X2=4) = 0.007 / (0.011 + 0.007)\n",
    "                \n",
    "                = 0.39\n",
    "               \n",
    "               = 39%\n",
    "                \n",
    "Since P(A|X1=3, X2=4) has the higher probability, Naive Bayes would predict the new instance to belong to class A."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
