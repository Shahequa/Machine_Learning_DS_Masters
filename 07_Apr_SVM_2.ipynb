{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8125df1-784a-4c60-a82a-76aca06ac898",
   "metadata": {},
   "source": [
    "## Support Vector Machine Assignment - 2\n",
    "**By Shahequa Modabbera**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a2e93-df05-4420-8286-01a54ec322bb",
   "metadata": {},
   "source": [
    "### Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68011d6a-362a-4ad5-8e72-ea1622b1a7d2",
   "metadata": {},
   "source": [
    "Ans) Polynomial functions and kernel functions are related in the context of machine learning algorithms, particularly in kernel methods such as Support Vector Machines (SVMs).\n",
    "\n",
    "Polynomial functions can be used as a type of kernel function in SVMs. Kernel functions are responsible for transforming the input data into a higher-dimensional feature space, where it may become easier to find a linear separation between classes. The transformed data is then used to train a linear classifier.\n",
    "\n",
    "In the case of polynomial kernel functions, they apply a polynomial transformation to the original input data. This transformation allows the SVM to capture nonlinear relationships between the features. The polynomial kernel function computes the similarity between two data points in the transformed feature space based on the polynomial expansion of their original feature vectors.\n",
    "\n",
    "The polynomial kernel function can be defined as:\n",
    "\n",
    "K(x, y) = (gamma * <x, y> + coef0)^degree\n",
    "\n",
    "where x and y are the input feature vectors, gamma is a parameter that controls the influence of each feature, coef0 is an additional parameter, and degree is the degree of the polynomial.\n",
    "\n",
    "In essence, the polynomial kernel function allows SVMs to implicitly operate in a higher-dimensional feature space without explicitly computing the transformed feature vectors. This allows SVMs to capture complex nonlinear relationships between the input features, making them more flexible and powerful for classification tasks.\n",
    "\n",
    "It's important to note that polynomial kernel functions are just one type of kernel function used in machine learning. There are other types of kernel functions, such as Gaussian (RBF) kernel, sigmoid kernel, and more, each with its own characteristics and applicability in different scenarios. The choice of the kernel function depends on the nature of the data and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd1ee72-af91-4d40-93a6-54c797352e59",
   "metadata": {},
   "source": [
    "### Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182a200-361c-418a-96de-7e075820397d",
   "metadata": {},
   "source": [
    "Ans) In Scikit-learn, implementing an SVM with a polynomial kernel is straightforward. We can use the `SVC` class with the `kernel='poly'` parameter to specify the polynomial kernel. Here's an example of how to implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82c50b5-3fa2-4c9b-a4a2-81ecd88ec887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with polynomial kernel\n",
    "svm = SVC(kernel='poly', degree=3)  # degree=3 is the default value for polynomial kernel\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d2359-4cde-46f8-9563-e8a2567d2992",
   "metadata": {},
   "source": [
    "In this example, we load the Iris dataset, split it into training and testing sets, create an SVM classifier with a polynomial kernel using `SVC(kernel='poly', degree=3)`, fit the classifier to the training data, make predictions on the testing data, and finally evaluate the accuracy of the classifier using the `accuracy_score` function.\n",
    "\n",
    "We can adjust the degree of the polynomial kernel by modifying the `degree` parameter in the `SVC` class. Higher values of the degree parameter will result in more complex decision boundaries. Additionally, you can tune other hyperparameters of the SVM, such as `C` for regularization strength and `gamma` for the kernel coefficient, to further optimize the performance of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c94887-e6e2-4e60-8fcb-4d221ad93b66",
   "metadata": {},
   "source": [
    "### Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd2ead7-6622-4d79-967b-abd631be0c3f",
   "metadata": {},
   "source": [
    "Ans) In Support Vector Regression (SVR), the value of epsilon (ε) is an important parameter that determines the width of the margin within which errors are tolerated. It represents the maximum allowable deviation of predicted values from the actual targets.\n",
    "\n",
    "Increasing the value of epsilon in SVR has an impact on the number of support vectors. Specifically:\n",
    "\n",
    "1. Larger Epsilon: When the value of epsilon is increased, it allows for a wider margin, which means that more data points can fall within the margin without contributing to the loss function. Consequently, more training instances may become support vectors as they fall within the wider margin, resulting in an increase in the number of support vectors.\n",
    "\n",
    "2. Smaller Epsilon: Conversely, when the value of epsilon is decreased, it tightens the margin, making it narrower. This requires the SVR model to fit the training instances more precisely, allowing fewer instances to be classified as support vectors. Consequently, reducing epsilon generally leads to a decrease in the number of support vectors.\n",
    "\n",
    "It's important to note that the number of support vectors affects the complexity and efficiency of the SVR model. Having a larger number of support vectors can result in longer training and prediction times, as well as increased memory requirements. On the other hand, reducing the number of support vectors can lead to a simpler model with potentially better generalization capabilities.\n",
    "\n",
    "The optimal value of epsilon depends on the specific problem and the trade-off between model complexity and accuracy. It is often determined through model selection techniques such as cross-validation or grid search, where different values of epsilon are tested and evaluated based on performance metrics such as mean squared error or R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e73e1-9b82-46d1-ba91-1aff61cefedc",
   "metadata": {},
   "source": [
    "### Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e42f81a-152f-4afd-b11e-72070ab7bf88",
   "metadata": {},
   "source": [
    "Ans) The choice of kernel function, C parameter, epsilon parameter, and gamma parameter in Support Vector Regression (SVR) can significantly impact the performance of the model. Let's discuss each parameter and its effect:\n",
    "\n",
    "1. Kernel Function:\n",
    "   - The kernel function determines the type of decision boundary that the SVR model will learn. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "   - The choice of kernel function depends on the underlying problem and the nature of the data. For example:\n",
    "     - Linear kernel: Suitable for linearly separable data or when the relationship between features and target variable is linear.\n",
    "     - RBF kernel: Suitable for nonlinear relationships and when there is no prior knowledge about the data distribution.\n",
    "     - Polynomial kernel: Suitable for capturing complex nonlinear relationships with polynomial features.\n",
    "   - Choosing the right kernel function is a crucial step in building an effective SVR model.\n",
    "\n",
    "2. C Parameter:\n",
    "   - The C parameter controls the trade-off between achieving a low training error and a low margin violation. It determines the penalty for misclassified or misfit data points.\n",
    "   - Larger C values result in a smaller margin and a higher penalty for violations, leading to a more complex model that fits the training data closely.\n",
    "   - Smaller C values allow for a larger margin and tolerate more margin violations, resulting in a simpler model with potentially better generalization.\n",
    "   - Increase C when you want a more complex model with high accuracy on the training data, but be cautious of overfitting. Decrease C when you want a simpler model with better generalization.\n",
    "\n",
    "3. Epsilon Parameter:\n",
    "   - The epsilon parameter (ε) represents the maximum allowable deviation of predicted values from the actual targets within the margin.\n",
    "   - Larger epsilon values allow for a wider margin and tolerate larger errors, resulting in a model that focuses more on the larger deviations.\n",
    "   - Smaller epsilon values tighten the margin and require predictions to be more precise, resulting in a model that focuses on smaller deviations.\n",
    "   - Increase epsilon when you want to allow more flexibility and accept larger errors. Decrease epsilon when you want a more precise model that penalizes larger errors.\n",
    "\n",
    "4. Gamma Parameter:\n",
    "   - The gamma parameter determines the influence of a single training example on the decision boundary.\n",
    "   - High gamma values result in a more complex decision boundary that is sensitive to individual data points, potentially leading to overfitting.\n",
    "   - Low gamma values result in a smoother decision boundary and a more general model.\n",
    "   - Increase gamma when you want the model to closely fit the training data, especially if there are complex relationships or many input features. Decrease gamma to reduce overfitting and improve generalization.\n",
    "\n",
    "The optimal values for these parameters depend on the specific problem, dataset, and trade-off between model complexity and generalization. It is recommended to perform hyperparameter tuning using techniques like grid search or random search to find the best combination of parameter values that optimize the performance of the SVR model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f494e9-c78e-4c62-b27f-a64896a79aad",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "- Import the necessary libraries and load the dataset\n",
    "- Split the dataset into training and testing set\n",
    "- Preprocess the data using any technique of your choice (e.g. scaling, normalization\n",
    "- Create an instance of the SVC classifier and train it on the training data\n",
    "- Use the trained classifier to predict the labels of the testing data\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "  precision, recall, F1-score\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "  improve its performance\n",
    "- Train the tuned classifier on the entire dataset\n",
    "- Save the trained classifier to a file for future use.\n",
    "\n",
    "#### Note: You can use any dataset of your choice for this assignment, but make sure it is suitable for classification and has a sufficient number of features and samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74786770-9e94-4bc8-9b6f-c9ccc2b7adf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_classifier.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels of the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "parameters = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}\n",
    "grid_search = GridSearchCV(svc, parameters)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "svc_tuned = grid_search.best_estimator_\n",
    "svc_tuned.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(svc_tuned, 'svm_classifier.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da83560-58a5-4cfe-a811-28e909dcd8cf",
   "metadata": {},
   "source": [
    "In this example, we first import the necessary libraries and load the Iris dataset. Then we split the dataset into training and testing sets using the `train_test_split` function. Next, we preprocess the data by scaling it using `StandardScaler`. We create an instance of the SVC classifier and train it on the scaled training data. We then use the trained classifier to predict labels for the testing data and evaluate its performance using the accuracy score.\n",
    "\n",
    "To improve the classifier's performance, we perform hyperparameter tuning using `GridSearchCV`. We define a set of hyperparameters to search over (in this case, different values of `C` and `gamma`) and find the best combination using cross-validation. We train the tuned classifier on the entire dataset to make use of all available data.\n",
    "\n",
    "Finally, we save the trained classifier to a file using the `joblib.dump` function for future use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
