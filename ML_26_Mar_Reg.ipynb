{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c508b702-c396-40f3-b05d-9147b63ce670",
   "metadata": {},
   "source": [
    "## Regression Assignment 1\n",
    "**By Shahequa Modabbera**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7650f6a-0277-4d38-b198-60e0469bfbbb",
   "metadata": {},
   "source": [
    "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da46ef9-221b-492b-8cd3-ef902ba79692",
   "metadata": {},
   "source": [
    "`Ans) Simple linear regression and multiple linear regression are both methods used in machine learning for predicting a numerical output variable based on one or more input variables.`\n",
    "\n",
    "`Simple linear regression involves predicting a single output variable (dependent variable) based on a single input variable (independent variable). The relationship between the two variables is assumed to be linear, and the goal is to find the best-fit line that minimizes the sum of the squared errors between the predicted values and the actual values. An example of simple linear regression is predicting the price of a house based on its size.`\n",
    "\n",
    "`Multiple linear regression, on the other hand, involves predicting a single output variable based on multiple input variables. The relationship between the input variables and the output variable can be linear or non-linear, and the goal is to find the best-fit hyperplane that minimizes the sum of the squared errors between the predicted values and the actual values. An example of multiple linear regression is predicting the price of a house based on its size, number of bedrooms, number of bathrooms, and location.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817fd3e5-ba11-447e-8005-46ac59d185ef",
   "metadata": {},
   "source": [
    "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986180bd-ba32-4827-b6ff-4486523174b3",
   "metadata": {},
   "source": [
    "`The main assumptions of linear regression are:`\n",
    "\n",
    "    Linearity: The relationship between the dependent variable and the independent variables should be linear. This means that the change in the dependent variable should be proportional to the change in the independent variables.\n",
    "\n",
    "    Independence: The observations should be independent of each other. This means that there should be no relationship between the residuals (the difference between the predicted values and the actual values) of one observation and the residuals of another observation.\n",
    "\n",
    "    Homoscedasticity: The variance of the residuals should be constant across all levels of the independent variables. This means that the spread of the residuals should be similar for all values of the independent variables.\n",
    "\n",
    "    Normality: The residuals should be normally distributed. This means that the distribution of the residuals should be symmetrical and bell-shaped.\n",
    "\n",
    "`To check whether these assumptions hold in a given dataset, we can use various diagnostic plots and statistical tests.`\n",
    "\n",
    "    Linearity: We can check for linearity by plotting the dependent variable against each independent variable. If the relationship appears to be non-linear, we may need to transform the data or use a non-linear regression model.\n",
    "\n",
    "    Independence: We can check for independence by plotting the residuals against the fitted values. If there is a clear pattern in the residuals, such as a curve or a U-shape, this indicates that the independence assumption is violated.\n",
    "\n",
    "    Homoscedasticity: We can check for homoscedasticity by plotting the residuals against the fitted values and looking for a constant spread of the residuals across all levels of the independent variables. If the spread of the residuals increases or decreases with the fitted values, this indicates that the homoscedasticity assumption is violated.\n",
    "\n",
    "    Normality: We can check for normality by plotting a histogram or a QQ plot of the residuals and looking for a symmetrical and bell-shaped distribution. We can also perform a normality test, such as the Shapiro-Wilk test or the Anderson-Darling test.\n",
    "\n",
    "`If any of these assumptions are violated, we may need to transform the data, use a different model, or adjust the parameters of the model to improve its performance.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ec4fc-8401-4f91-afef-b979be18223e",
   "metadata": {},
   "source": [
    "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1065f0-9d5a-4f59-8c5d-a4bf8978b7d9",
   "metadata": {},
   "source": [
    "`Ans)`\n",
    "`In a linear regression model, the slope and intercept represent the relationship between the input variable (independent variable) and the output variable (dependent variable). The slope represents the change in the output variable for every one-unit change in the input variable, while the intercept represents the value of the output variable when the input variable is zero.`\n",
    "\n",
    "**For example, consider a linear regression model that predicts the salary of an employee based on their years of experience. The model equation can be written as:**\n",
    "\n",
    "`salary = intercept + slope * years of experience`\n",
    "\n",
    "**If the slope is 5000, it means that for every additional year of experience, the salary is expected to increase by Rs.5000. If the intercept is 30000, it means that the expected salary for an employee with zero years of experience is Rs.30,000.**\n",
    "\n",
    "**So, if an employee has 5 years of experience, the predicted salary can be calculated as follows:**\n",
    "\n",
    "`salary = 30000 + 5000 * 5 = Rs.55,000`\n",
    "\n",
    "**In this example, the intercept represents the base salary for an employee with no experience, while the slope represents the additional salary an employee can expect to receive for each additional year of experience.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5cac4-e1dd-4cb6-87d8-5abcc6941306",
   "metadata": {},
   "source": [
    "### Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd85ede-ea33-4916-9d67-ef92db671dde",
   "metadata": {},
   "source": [
    "`Ans) Gradient descent is an optimization algorithm used in machine learning to minimize the cost function of a model by iteratively adjusting its parameters in the opposite direction of the gradient. The gradient is the slope of the cost function, and by moving in the direction of the negative gradient, the algorithm can converge to the optimal set of parameters that best fits the training data.` \n",
    "\n",
    "`Gradient descent can be applied to a wide range of machine learning algorithms, including linear regression, logistic regression, neural networks, and support vector machines. These algorithms use gradient descent to optimize their respective cost functions and improve their performance on the training data. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e27e3-233a-49d2-bf78-15bc9eed63be",
   "metadata": {},
   "source": [
    "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542dedab-99c3-411e-9c0e-1e076f886122",
   "metadata": {},
   "source": [
    "`Ans) Multiple linear regression is an extension of simple linear regression where we have more than one input variable to predict the output variable. In other words, multiple linear regression predicts a dependent variable based on the values of multiple independent variables. It can be represented mathematically as:`\n",
    "\n",
    "`Y = β0 + β1X1 + β2X2 + … + βn*Xn + ε`\n",
    "\n",
    "**Where,**\n",
    "\n",
    "    Y is the dependent variable that we want to predict\n",
    "    \n",
    "    β0 is the intercept (the value of Y when all the independent variables are 0)\n",
    "    \n",
    "    β1, β2, … βn are the coefficients of the independent variables X1, X2, … Xn respectively\n",
    "    \n",
    "    ε is the error term, which represents the unexplained variation in the dependent variable\n",
    "\n",
    "`Compared to simple linear regression, multiple linear regression is more flexible and allows us to account for the effects of multiple independent variables on the dependent variable. This makes it a more powerful tool for predicting outcomes in complex scenarios where multiple factors may be influencing the outcome.`\n",
    "\n",
    "`However, the increased complexity of multiple linear regression also makes it more difficult to interpret the relationship between each independent variable and the dependent variable. Additionally, the more independent variables we include in the model, the greater the risk of overfitting and the more challenging it becomes to find the optimal values of the coefficients that minimize the error term.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85d4d42-cb5b-4525-a486-c68b2cb6a30c",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a89b6b-f413-4b17-886f-c82baa863bcc",
   "metadata": {},
   "source": [
    "`Ans) Multicollinearity is a phenomenon in multiple linear regression where two or more independent variables are highly correlated with each other. This can cause problems in the model because it becomes difficult to separate the effects of each independent variable on the dependent variable, which can lead to inaccurate coefficient estimates and reduced predictive power.`\n",
    "\n",
    "`One way to detect multicollinearity is to look at the correlation matrix of the independent variables. If two or more variables have a correlation coefficient close to 1 or -1, then there may be multicollinearity present. Another approach is to calculate the Variance Inflation Factor (VIF) for each independent variable, which measures the degree to which that variable can be predicted by the other independent variables in the model. A high VIF indicates that the variable may be influenced by multicollinearity.`\n",
    "\n",
    "**To address multicollinearity, there are several techniques that can be used, including:**\n",
    "\n",
    "    Feature selection: Identifying and removing redundant or correlated variables from the model can help to reduce the impact of multicollinearity.\n",
    "\n",
    "    Principal Component Analysis (PCA): This is a technique that transforms the original variables into a new set of variables that are uncorrelated with each other. These new variables, known as principal components, can be used as input variables in the regression model.\n",
    "\n",
    "    Ridge Regression: Ridge regression is a regularization technique that adds a penalty term to the regression equation to discourage large coefficient values. This can help to reduce the impact of multicollinearity by shrinking the coefficient estimates towards zero.\n",
    "\n",
    "    Lasso Regression: Lasso regression is another regularization technique that adds a penalty term to the regression equation, but it uses a different penalty function that can set some of the coefficients to zero. This can help to remove redundant variables from the model and reduce the impact of multicollinearity.\n",
    "\n",
    "`By detecting and addressing multicollinearity, we can improve the accuracy and interpretability of our multiple linear regression models.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb8e282-e1b8-4982-ae39-444255abd9cb",
   "metadata": {},
   "source": [
    "### Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757677f0-dbfc-47f6-a7ce-cda7d0b740c8",
   "metadata": {},
   "source": [
    "`Ans) Polynomial regression is a type of regression analysis used to model the relationship between a dependent variable and one or more independent variables. It is similar to linear regression, but instead of fitting a straight line to the data, it fits a curved line, which is defined by a polynomial equation.`\n",
    "\n",
    "`The main difference between polynomial regression and linear regression is that in polynomial regression, the relationship between the independent and dependent variables is modeled as an nth degree polynomial, whereas in linear regression, the relationship is modeled as a linear function.`\n",
    "\n",
    "**For example, let's say we have a dataset that shows the relationship between the number of years of education and the income level of a group of people. In linear regression, we would fit a straight line to the data, with the number of years of education as the independent variable and income as the dependent variable. In polynomial regression, we might fit a curved line to the data, which could better capture the relationship between education and income, especially if the relationship is not strictly linear.**\n",
    "\n",
    "`In polynomial regression, we can choose the degree of the polynomial equation we want to use. A polynomial equation of degree one is a linear equation, while a polynomial equation of degree two is a quadratic equation, and so on. Choosing the appropriate degree of the polynomial equation is important to balance between underfitting and overfitting.`\n",
    "\n",
    "`While polynomial regression can be more flexible than linear regression, it can also be more complex and can lead to overfitting if the degree of the polynomial is too high.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88fc1eb-5db3-40cf-b429-83f442bb6a57",
   "metadata": {},
   "source": [
    "### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2432c9-c13d-44cf-9d41-9f9f26423e37",
   "metadata": {},
   "source": [
    "`Ans) Advantages of polynomial regression compared to linear regression:`\n",
    "\n",
    "* Polynomial regression can model more complex nonlinear relationships between variables, whereas linear regression can only model linear relationships.\n",
    "    \n",
    "* It can capture curvature and turning points in the data, making it a better fit for some datasets.\n",
    "    \n",
    "* It can improve the accuracy of the model predictions by fitting the data more closely.\n",
    "\n",
    "`Disadvantages of polynomial regression compared to linear regression:`\n",
    "\n",
    "* It can be more computationally expensive to fit, especially for higher-order polynomials.\n",
    "    \n",
    "* It can be more prone to overfitting the data, especially for higher-order polynomials, which can result in poor generalization to new data.\n",
    "    \n",
    "`Situations in which polynomial regression may be preferred over linear regression:`\n",
    "\n",
    "* When the relationship between the variables is expected to be nonlinear or have curvature.\n",
    "    \n",
    "* When a higher degree of accuracy is required in the predictions.\n",
    "    \n",
    "* When the dataset is small and there is a need to capture more information from the available data.\n",
    "\n",
    "`It is important to note that the choice of regression model should be based on the nature of the problem, the available data, and the goals of the analysis. A thorough understanding of the data and the underlying relationships between variables is crucial in selecting the appropriate regression model.`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
